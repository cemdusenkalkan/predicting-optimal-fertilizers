{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fixed Parameters XGBoost Model for Fertilizer Recommendation\n",
    "\n",
    "This notebook uses the parameters from the overfitted trial but with proper validation:\n",
    "- **Parameters**: max_depth=7, learning_rate=0.254, n_estimators=2330, etc.\n",
    "- **Same techniques**: Categorical features, NPK ratios, data expansion\n",
    "- **No Optuna**: Direct training with fixed parameters\n",
    "- **Proper CV**: To get realistic performance estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/playground-series-s5e6'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP@3 implementation\n",
    "def apk(actual, predicted, k=3):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    \n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=3):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def map3_score_from_proba(y_true, y_pred_proba):\n",
    "    top3_indices = np.argsort(y_pred_proba, axis=1)[:, ::-1][:, :3]\n",
    "    \n",
    "    map3_scores = []\n",
    "    for i, true_label in enumerate(y_true):\n",
    "        predicted_labels = top3_indices[i]\n",
    "        map3_scores.append(apk([true_label], predicted_labels, k=3))\n",
    "    \n",
    "    return np.mean(map3_scores)\n",
    "\n",
    "print(\"MAP@3 evaluation functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e6/sample_submission.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ultra_competitive_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fix column name typo\n",
    "    if 'Temparature' in df.columns:\n",
    "        df = df.rename(columns={'Temparature': 'Temperature'})\n",
    "    \n",
    "    # 1. CATEGORICAL VERSIONS (+0.006)\n",
    "    numerical_cols = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Phosphorous', 'Potassium']\n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_cat'] = pd.cut(df[col], bins=20, labels=False, duplicates='drop')\n",
    "        \n",
    "    # 2. CONSTANT FEATURE (+0.005)\n",
    "    df['const'] = 1\n",
    "    \n",
    "    # 3. ENVIRONMENTAL FEATURES\n",
    "    env_cols = [col for col in ['Temperature', 'Humidity', 'Moisture'] if col in df.columns]\n",
    "    if len(env_cols) >= 2:\n",
    "        df['env_max'] = df[env_cols].max(axis=1)\n",
    "        df['env_min'] = df[env_cols].min(axis=1)\n",
    "        df['env_range'] = df['env_max'] - df['env_min']\n",
    "        df['climate_comfort'] = df[env_cols].mean(axis=1)\n",
    "    \n",
    "    if 'Temperature' in df.columns and 'Humidity' in df.columns:\n",
    "        df['temp_humidity_index'] = df['Temperature'] * df['Humidity'] / 100\n",
    "    \n",
    "    # 4. NPK RATIOS (CRITICAL)\n",
    "    epsilon = 1e-8\n",
    "    npk_cols = ['Nitrogen', 'Phosphorous', 'Potassium']\n",
    "    \n",
    "    if all(col in df.columns for col in npk_cols):\n",
    "        df['N_P_ratio'] = df['Nitrogen'] / (df['Phosphorous'] + epsilon)\n",
    "        df['N_K_ratio'] = df['Nitrogen'] / (df['Potassium'] + epsilon)\n",
    "        df['P_K_ratio'] = df['Phosphorous'] / (df['Potassium'] + epsilon)\n",
    "        df['Total_NPK'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']\n",
    "        df['NPK_balance'] = df[npk_cols].std(axis=1)\n",
    "        \n",
    "        # Clip extreme ratios\n",
    "        ratio_cols = ['N_P_ratio', 'N_K_ratio', 'P_K_ratio']\n",
    "        for col in ratio_cols:\n",
    "            df[col] = np.clip(df[col], 0, 100)\n",
    "            \n",
    "        # NPK dominance features\n",
    "        df['N_dominance'] = df['Nitrogen'] / (df['Total_NPK'] + epsilon)\n",
    "        df['P_dominance'] = df['Phosphorous'] / (df['Total_NPK'] + epsilon)  \n",
    "        df['K_dominance'] = df['Potassium'] / (df['Total_NPK'] + epsilon)\n",
    "        \n",
    "    # 5. TEMPERATURE SUITABILITY\n",
    "    if 'Temperature' in df.columns and 'Crop Type' in df.columns:\n",
    "        crop_temp_map = {\n",
    "            'Sugarcane': (26, 35), 'Maize': (25, 32), 'Wheat': (20, 30),\n",
    "            'Paddy': (25, 35), 'Cotton': (25, 35), 'Tobacco': (20, 30),\n",
    "            'Barley': (15, 25), 'Millets': (25, 35), 'Pulses': (20, 30),\n",
    "            'Oil seeds': (20, 30), 'Ground Nuts': (25, 32)\n",
    "        }\n",
    "        \n",
    "        def temp_suitable(row):\n",
    "            temp_range = crop_temp_map.get(row['Crop Type'], (25, 32))\n",
    "            return 1 if temp_range[0] <= row['Temperature'] <= temp_range[1] else 0\n",
    "            \n",
    "        df['temp_suitability'] = df.apply(temp_suitable, axis=1)\n",
    "    \n",
    "    # 6. CROP-SOIL INTERACTIONS\n",
    "    if 'Crop Type' in df.columns and 'Soil Type' in df.columns:\n",
    "        df['Crop_Soil_combo'] = df['Crop Type'].astype(str) + '_' + df['Soil Type'].astype(str)\n",
    "    \n",
    "    # 7. STRESS INDICATORS\n",
    "    if 'Temperature' in df.columns:\n",
    "        df['temp_stress'] = np.where((df['Temperature'] < 20) | (df['Temperature'] > 35), 1, 0)\n",
    "    if 'Humidity' in df.columns:\n",
    "        df['humidity_stress'] = np.where((df['Humidity'] < 40) | (df['Humidity'] > 80), 1, 0)\n",
    "    if 'Moisture' in df.columns:\n",
    "        df['moisture_stress'] = np.where((df['Moisture'] < 30) | (df['Moisture'] > 70), 1, 0)\n",
    "    \n",
    "    # 8. NUTRIENT EFFICIENCY\n",
    "    if all(col in df.columns for col in npk_cols):\n",
    "        df['nutrient_efficiency'] = df['Total_NPK'] / (df['Temperature'] + df['Humidity'] + df['Moisture'] + epsilon)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Ultra-competitive feature engineering function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "X_train = train_df.drop(['id', 'Fertilizer Name'], axis=1)\n",
    "y_train = train_df['Fertilizer Name']\n",
    "\n",
    "X_train_engineered = create_ultra_competitive_features(X_train)\n",
    "\n",
    "# Label encode target\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Engineered features: {X_train_engineered.shape[1]}\")\n",
    "print(f\"Target classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Handle categorical columns\n",
    "categorical_cols = ['Soil Type', 'Crop Type', 'Crop_Soil_combo']\n",
    "for col in categorical_cols:\n",
    "    if col in X_train_engineered.columns:\n",
    "        X_train_engineered[col] = X_train_engineered[col].astype('category')\n",
    "\n",
    "print(\"Feature engineering completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4x Data expansion\n",
    "print(\"Expanding training data by factor of 4...\")\n",
    "X_train_expanded = pd.concat([X_train_engineered] * 4, ignore_index=True)\n",
    "y_train_expanded = np.tile(y_train_encoded, 4)\n",
    "\n",
    "print(f\"Original size: {len(y_train_encoded)} -> Expanded size: {len(y_train_expanded)}\")\n",
    "print(\"Data expansion completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters from overfitted trial (but with proper validation)\n",
    "FIXED_PARAMS = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 7,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'enable_categorical': True,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,\n",
    "    'gpu_id': 0,\n",
    "    \n",
    "    # Parameters from overfitted trial\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.2536999076681772,\n",
    "    'n_estimators': 2330,\n",
    "    'subsample': 0.8394633936788146,\n",
    "    'colsample_bytree': 0.6624074561769746,\n",
    "    'colsample_bylevel': 0.662397808134481,\n",
    "    'reg_alpha': 0.014936568554617643,\n",
    "    'reg_lambda': 3.9676050770529883,\n",
    "    'min_child_weight': 7,\n",
    "    'gamma': 3.540362888980227\n",
    "}\n",
    "\n",
    "print(\"Fixed parameters loaded from overfitted trial:\")\n",
    "for param, value in FIXED_PARAMS.items():\n",
    "    if param not in ['objective', 'num_class', 'eval_metric', 'tree_method', 'enable_categorical', 'random_state', 'verbosity', 'gpu_id']:\n",
    "        print(f\"  {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper cross-validation with fixed parameters\n",
    "print(\"Performing 5-fold cross-validation with fixed parameters...\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros((len(y_train_expanded), 7))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_expanded, y_train_expanded)):\n",
    "    print(f\"Training fold {fold + 1}/5...\")\n",
    "    \n",
    "    X_tr, X_val = X_train_expanded.iloc[train_idx], X_train_expanded.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_expanded[train_idx], y_train_expanded[val_idx]\n",
    "    \n",
    "    # Train model with fixed parameters\n",
    "    model = xgb.XGBClassifier(**FIXED_PARAMS)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predict and calculate MAP@3\n",
    "    y_pred_proba = model.predict_proba(X_val)\n",
    "    oof_predictions[val_idx] = y_pred_proba\n",
    "    \n",
    "    map3_score = map3_score_from_proba(y_val, y_pred_proba)\n",
    "    cv_scores.append(map3_score)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} MAP@3: {map3_score:.6f}\")\n",
    "\n",
    "# Overall CV performance\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "std_cv_score = np.std(cv_scores)\n",
    "oof_score = map3_score_from_proba(y_train_expanded, oof_predictions)\n",
    "\n",
    "print(f\"\\nðŸ“Š Cross-Validation Results:\")\n",
    "print(f\"  Mean CV MAP@3: {mean_cv_score:.6f} Â± {std_cv_score:.6f}\")\n",
    "print(f\"  OOF MAP@3: {oof_score:.6f}\")\n",
    "print(f\"  Individual folds: {[f'{score:.6f}' for score in cv_scores]}\")\n",
    "\n",
    "if mean_cv_score > 0.40:\n",
    "    print(f\"  âš ï¸ HIGH SCORE - Likely overfitted!\")\n",
    "elif mean_cv_score > 0.38:\n",
    "    print(f\"  ðŸ† EXCELLENT - Competitive performance!\")\n",
    "elif mean_cv_score > 0.35:\n",
    "    print(f\"  âœ… GOOD - Solid performance!\")\n",
    "else:\n",
    "    print(f\"  ðŸ“ˆ MODERATE - Room for improvement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final ensemble (5 models with different seeds)\n",
    "print(\"Training final ensemble with 5 different seeds...\")\n",
    "\n",
    "final_models = []\n",
    "seeds = [42, 123, 456, 789, 999]\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "    print(f\"Training model {i+1}/5 with seed {seed}...\")\n",
    "    \n",
    "    # Update random seed\n",
    "    params = FIXED_PARAMS.copy()\n",
    "    params['random_state'] = seed\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train_expanded, y_train_expanded, verbose=False)\n",
    "    \n",
    "    final_models.append(model)\n",
    "\n",
    "print(f\"âœ… Ensemble of {len(final_models)} models trained!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data and make predictions\n",
    "print(\"Preparing test data...\")\n",
    "\n",
    "X_test = test_df.drop('id', axis=1, errors='ignore')\n",
    "X_test_engineered = create_ultra_competitive_features(X_test)\n",
    "\n",
    "# Handle categorical variables\n",
    "for col in categorical_cols:\n",
    "    if col in X_test_engineered.columns:\n",
    "        X_test_engineered[col] = X_test_engineered[col].astype('category')\n",
    "\n",
    "print(f\"Test data shape after engineering: {X_test_engineered.shape}\")\n",
    "\n",
    "# Make ensemble predictions\n",
    "print(\"Making ensemble predictions...\")\n",
    "\n",
    "all_test_predictions = []\n",
    "for i, model in enumerate(final_models):\n",
    "    print(f\"Predicting with model {i+1}/{len(final_models)}...\")\n",
    "    pred = model.predict_proba(X_test_engineered)\n",
    "    all_test_predictions.append(pred)\n",
    "\n",
    "# Ensemble average\n",
    "test_probabilities = np.mean(all_test_predictions, axis=0)\n",
    "\n",
    "# Get top 3 predictions\n",
    "top3_predictions = np.argsort(test_probabilities, axis=1)[:, ::-1][:, :3]\n",
    "\n",
    "# Convert back to fertilizer names\n",
    "top3_fertilizer_names = []\n",
    "for i in range(len(top3_predictions)):\n",
    "    fertilizer_names = [label_encoder.inverse_transform([pred])[0] for pred in top3_predictions[i]]\n",
    "    top3_fertilizer_names.append(fertilizer_names)\n",
    "\n",
    "print(\"âœ… Predictions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id']\n",
    "\n",
    "# Correct format: space-separated top 3 predictions in single column\n",
    "submission['Fertilizer Name'] = [' '.join(pred) for pred in top3_fertilizer_names]\n",
    "\n",
    "print(\"\\nðŸ“‹ First 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Save submission\n",
    "submission_filename = '/kaggle/working/fixed_params_submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "print(f\"\\nðŸ’¾ Submission saved as: {submission_filename}\")\n",
    "\n",
    "# Also save backup\n",
    "submission.to_csv('fixed_params_submission.csv', index=False)\n",
    "print(\"ðŸ’¾ Backup submission saved to current directory\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nðŸ“ˆ Fixed Parameters Model Performance Summary:\")\n",
    "print(f\"  â€¢ CV MAP@3: {mean_cv_score:.6f} Â± {std_cv_score:.6f}\")\n",
    "print(f\"  â€¢ OOF MAP@3: {oof_score:.6f}\")\n",
    "print(f\"  â€¢ Features used: {X_train_engineered.shape[1]}\")\n",
    "print(f\"  â€¢ Training samples: {len(y_train_expanded):,} (4x expanded)\")\n",
    "print(f\"  â€¢ Ensemble models: {len(final_models)}\")\n",
    "print(f\"  â€¢ Test predictions: {len(submission):,}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Expected vs Overfitted Performance:\")\n",
    "print(f\"  â€¢ Overfitted trial score: 0.460 (unrealistic)\")\n",
    "print(f\"  â€¢ Proper CV score: {mean_cv_score:.6f} (realistic)\")\n",
    "print(f\"  â€¢ Champion score to beat: 0.383\")\n",
    "\n",
    "if mean_cv_score >= 0.383:\n",
    "    print(f\"  ðŸ† LIKELY TO BEAT CHAMPION SCORE!\")\n",
    "elif mean_cv_score >= 0.380:\n",
    "    print(f\"  ðŸ¥ˆ VERY COMPETITIVE!\")\n",
    "elif mean_cv_score >= 0.375:\n",
    "    print(f\"  ðŸ¥‰ STRONG PERFORMANCE!\")\n",
    "else:\n",
    "    print(f\"  ðŸ“ˆ REASONABLE PERFORMANCE\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
